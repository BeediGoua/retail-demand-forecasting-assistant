{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Challenger Model V2: Advanced Optimization\n",
                "\n",
                "**Goal**: Push the limits of the CatBoost Challenger model (WAPE < 0.25).\n",
                "\n",
                "**Advanced Strategies (V2)**:\n",
                "1.  **Hyperparameter Optimization**: Using **Optuna** to find the perfect `learning_rate`, `depth`, and `l2_leaf_reg`.\n",
                "2.  **Advanced Features**: Adding Rolling Means (e.g., avg sales of last 4 same-weekdays) to capture finer seasonality.\n",
                "3.  **Ensembling**: Stacking CatBoost with a simple Ridge Regression to blend non-linear and linear signals."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from catboost import CatBoostRegressor, Pool\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.metrics import mean_squared_error\n",
                "import optuna\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add root to path for imports\n",
                "sys.path.append('..')\n",
                "from src.features.features import RetailFeatureEngineer, create_lags\n",
                "\n",
                "pd.set_option('display.max_columns', None)\n",
                "sns.set_theme(style=\"whitegrid\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load & Enrich Data (Advanced Features)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load optimized daily data\n",
                "df = pd.read_parquet('../data/processed/daily_canon.parquet')\n",
                "df = df.sort_values(by=['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
                "\n",
                "# --- FEATURE ENGINEERING V2 ---\n",
                "print(\"Generating V2 Features...\")\n",
                "\n",
                "# 1. Base Lags (Standard)\n",
                "df = create_lags(df, lags=[7, 14, 28])\n",
                "\n",
                "# 2. [NEW] Rolling Features (Window Statistics)\n",
                "# Logic: Average of sales over last 28 days (excluding current)\n",
                "# Groupby is slow, so we iterate carefully or rely on shift + rolling\n",
                "# Approximation: Rolling mean of Lag-7 (to avoid data leakage)\n",
                "df['sales_roll_mean_28'] = df.groupby(['store_nbr', 'family'])['sales_lag_7'].transform(lambda x: x.rolling(28).mean())\n",
                "df['sales_roll_std_7'] = df.groupby(['store_nbr', 'family'])['sales_lag_7'].transform(lambda x: x.rolling(7).std())\n",
                "\n",
                "# 3. Standard Pillars (Payday, Earthquake, Clusters)\n",
                "engineer = RetailFeatureEngineer()\n",
                "df = engineer.transform(df)\n",
                "\n",
                "# Clean NaNs\n",
                "df = df.dropna(subset=['sales_lag_28', 'sales_roll_mean_28', 'sales'])\n",
                "\n",
                "print(f\"V2 Dataset Ready: {df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prepare Splits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "split_date = '2017-08-01'\n",
                "mask_train = df['date'] < split_date\n",
                "mask_val = (df['date'] >= split_date) & (df['is_train_day'] == 1)\n",
                "\n",
                "drop_cols = ['sales', 'date', 'id', 'set', 'transactions', 'transactions_missing']\n",
                "X = df.drop(columns=drop_cols)\n",
                "y = df['sales']\n",
                "\n",
                "cat_features = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster']\n",
                "# Ensure categories are strings for CatBoost\n",
                "for c in cat_features:\n",
                "    X[c] = X[c].astype(str)\n",
                "\n",
                "X_train = X[mask_train]\n",
                "y_train = y[mask_train]\n",
                "X_val = X[mask_val]\n",
                "y_val = y[mask_val]\n",
                "\n",
                "print(f\"Train: {len(X_train)} rows | Validation: {len(X_val)} rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Optuna Hyperparameter Tuning\n",
                "Optimizing CatBoost parameters with **Tweedie** loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial):\n",
                "    params = {\n",
                "        'iterations': 500, # Keep low for fast tuning\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
                "        'depth': trial.suggest_int('depth', 4, 10),\n",
                "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
                "        'loss_function': 'Tweedie:variance_power=1.5',\n",
                "        'eval_metric': 'RMSE',\n",
                "        'random_seed': 42,\n",
                "        'verbose': False,\n",
                "        'allow_writing_files': False\n",
                "    }\n",
                "\n",
                "    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
                "    val_pool = Pool(X_val, y_val, cat_features=cat_features)\n",
                "\n",
                "    model = CatBoostRegressor(**params)\n",
                "    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=20)\n",
                "\n",
                "    preds = model.predict(X_val)\n",
                "    preds = np.maximum(preds, 0)\n",
                "    \n",
                "    # Optimize for WAPE directly inside Optuna\n",
                "    wape = np.sum(np.abs(y_val - preds)) / np.sum(y_val)\n",
                "    return wape\n",
                "\n",
                "# Run Optimization\n",
                "print(\"Starting Optuna Study (5 Trials for Demo)...\")\n",
                "study = optuna.create_study(direction='minimize')\n",
                "study.optimize(objective, n_trials=5) # Increase n_trials for real impact\n",
                "\n",
                "print(f\"Best WAPE: {study.best_value:.4f}\")\n",
                "print(f\"Best Params: {study.best_params}\")\n",
                "\n",
                "best_params = study.best_params\n",
                "# Add fixed params back\n",
                "best_params['iterations'] = 1000 # Production iterations\n",
                "best_params['loss_function'] = 'Tweedie:variance_power=1.5'\n",
                "best_params['eval_metric'] = 'RMSE'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Stacking (Ensemble)\n",
                "Combine Best CatBoost with a robust linear model (Ridge)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Train Best CatBoost\n",
                "model_cb = CatBoostRegressor(**best_params, verbose=100)\n",
                "model_cb.fit(X_train, y_train, cat_features=cat_features)\n",
                "preds_cb = model_cb.predict(X_val)\n",
                "\n",
                "# 2. Train Simple Ridge (Linear)\n",
                "# Ridge needs numeric input only, drop categories or encode\n",
                "X_train_lin = X_train.select_dtypes(include=[np.number]).fillna(0)\n",
                "X_val_lin = X_val.select_dtypes(include=[np.number]).fillna(0)\n",
                "\n",
                "model_ridge = Ridge(alpha=1.0)\n",
                "model_ridge.fit(X_train_lin, y_train)\n",
                "preds_ridge = model_ridge.predict(X_val_lin)\n",
                "\n",
                "# 3. Blending (Stacking)\n",
                "# Weighted average: 80% CatBoost (Strong), 20% Ridge (Robust)\n",
                "final_preds = (0.8 * preds_cb) + (0.2 * preds_ridge)\n",
                "final_preds = np.maximum(final_preds, 0)\n",
                "\n",
                "wape_blend = np.sum(np.abs(y_val - final_preds)) / np.sum(y_val)\n",
                "print(f\"---\\nCatBoost Only WAPE: {np.sum(np.abs(y_val - preds_cb))/np.sum(y_val):.4f}\")\n",
                "print(f\"Ridge Only WAPE: {np.sum(np.abs(y_val - preds_ridge))/np.sum(y_val):.4f}\")\n",
                "print(f\"Stacking (80/20) WAPE: {wape_blend:.4f}\\n---\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}